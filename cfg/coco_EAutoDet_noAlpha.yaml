root_path: &root_path runs/coco_EAutoDet-s_noAlpha_nogumbelSamper/
epoch: &epoch 200
accumulate_gradient: &acc_grad 1

data: &data cfg/dataset/coco_yolo.yaml
model: &model cfg/model/EAutoDet-s_noAlpha_nogumbel.yaml
pretrain: &pretrain null #runs/coco_EAutoDet-s_noAlpha_nogumbelSampler/ckpt/last.pt

criterion: &criterion 
  submodule_name: src.criterions.YOLOv5Loss
  args:
    hyp:
      box: 0.05  # box loss gain
      cls: 0.5  # cls loss gain
      cls_pw: 1.0  # cls BCELoss positive_weight
      obj: 1.0  # obj loss gain (scale with pixels)
      obj_pw: 1.0  # obj BCELoss positive_weight
      iou_t: 0.20  # IoU training threshold
      anchor_t: 4.0  # anchor-multiple threshold
      fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)
    img_size: !cross_ref [!join [*data, ':input_size']]
    strides: !cross_ref [!join [*model, ':strides']]
    anchors: !cross_ref [!join [*model, ':anchors']]
    iou_loss_ratio: 1.0
    autobalance: False

train_hyp: &hyp
  lr0: &lr0 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)
  lrf: &lrf 0.2  # final OneCycleLR learning rate (lr0 * lrf)
  momentum: &momentum 0.937  # SGD momentum/Adam beta1
  weight_decay: &wd !expr ['0.0005/64*', !cross_ref [!join [*data, ":batch_size"]]]  # optimizer weight decay 5e-4 * bs / 64
  warmup_epochs: 3.0  # warmup epochs (fractions ok)
  warmup_init_momentum: &warmup_init_momentum 0.8 
  warmup_bias_lr: &warmup_bias_lr 0.1  


optimizer: &optimizer
  submodule_name: torch.optim.SGD
  args:
    params: [{'params': bn_parameters}, {'params': bias_parameters}, {'params': all_parameters, 'weight_decay': *wd}]
    lr: *lr0
    momentum: *momentum
    nesterov: True

lr_scheduler: &scheduler
  submodule_name: torch.optim.lr_scheduler.LambdaLR
  args:
    optimizer: 
    lr_lambda: !get_module [src.scheduler.one_cycle, {'start': 1, 'end': *lrf, 'steps': *epoch}]


hooks: &hooks
  opt_hook:
    submodule_name: OptHOOK
    args:
      priority: 0
      accumulate_gradient: *acc_grad
  
  lr_scheduler_hook:
    submodule_name: LrScheduleHOOK
    args: 
      priority: 5
      mode: epoch

  eval_hook:
    submodule_name: EvalCOCOmAPHOOK
    args: 
      anno_file: 'data/coco2017/annotations/instances_val2017.json'
      iou_thres: 0.6
      conf_thres: 0.001
      eval_by_cocotools: True
      priority: 10
  
  ddp_hook:
    submodule_name: DDPHOOK
    args:
      priority: 10
  
  
  log_hook:
    submodule_name: LogHOOK
    args:
      logger_name: TrainPip
      log_path: !join [*root_path, train.log]
      print_freq: 100
      only_master: True
      priority: 30
  
  ckpt_hook:
    submodule_name: CkptHOOK
    args:
      save_root: !join [*root_path, ckpt/]
      pretrain: *pretrain
      only_master: True
      priority: 30
  
  warmup_hook:
    submodule_name: WarmupHOOK
    args:
      max_epoch: 3 
      max_iter: 1000
      warmup_init_lr: [0, *warmup_bias_lr, 0]
      warmup_init_momentum: *warmup_init_momentum
      priority: 0
      accumulate_gradient: *acc_grad
  
  ema_hook:
    submodule_name: EMAHOOK
    args:
      decay: 0.9999
      priority: 5
      accumulate_gradient: *acc_grad


engine:
  submodule_name: engines.NNEngine
  args:
    data: !cross_ref [*data]
    criterion: *criterion
    model: !cross_ref [*model]
    optimizer: *optimizer
    lr_scheduler: *scheduler
    hooks: *hooks
    amp: True
    amp_val: True



